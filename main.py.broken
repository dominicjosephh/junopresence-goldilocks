import os
import json
import base64
import requests
import random
import re
import hashlib
import time
import subprocess
import threading
from datetime import datetime
from functools import lru_cache
from fastapi import FastAPI, UploadFile, Form, Request
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv
import uvicorn

# Import our music command parser
from music_command_parser import MusicCommandParser, SpotifyController, MusicIntent

# üéôÔ∏è Import speech recognition
from speech_service import get_speech_service

load_dotenv()
ELEVENLABS_API_KEY = os.getenv('ELEVENLABS_API_KEY')
voice_id = os.getenv('ELEVENLABS_VOICE_ID')

# Add Spotify credentials
SPOTIFY_CLIENT_ID = os.getenv('SPOTIFY_CLIENT_ID')
SPOTIFY_CLIENT_SECRET = os.getenv('SPOTIFY_CLIENT_SECRET')

# üöÄ ADD TOGETHER AI CREDENTIALS
TOGETHER_AI_API_KEY = os.getenv('TOGETHER_AI_API_KEY')
TOGETHER_AI_BASE_URL = "https://api.together.xyz/v1"

MEMORY_FILE = 'memory.json'
FACTS_LIMIT = 20
CHAT_LOG_FILE = "chat_log.json"
AUDIO_DIR = "static"
AUDIO_FILENAME = "juno_response.mp3"
AUDIO_PATH = os.path.join(AUDIO_DIR, AUDIO_FILENAME)

# üöÄ FIXED LLAMA.CPP CONFIGURATION
LLAMA_CPP_PATH = "/opt/build/bin/llama-cli"
MODEL_PATH = "/opt/models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

# Performance optimization globals
RESPONSE_CACHE = {}
CACHE_MAX_SIZE = 50
CACHE_TTL = 3600  # 1 hour

# Model management
MODEL_LOADED = False
MODEL_LOCK = threading.Lock()

# üöÄ AI PROVIDER PREFERENCES
USE_TOGETHER_AI_FIRST = os.getenv('USE_TOGETHER_AI_FIRST', 'false').lower() == 'true'
TOGETHER_AI_TIMEOUT = 15  # seconds

# Initialize music intelligence
music_parser = MusicCommandParser()
spotify_controller = SpotifyController(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)

# Ensure static folder exists
os.makedirs(AUDIO_DIR, exist_ok=True)

def get_cache_key(prompt, chat_history_str="", voice_mode="Base"):
    """Generate cache key for responses"""
    combined = f"{prompt}:{chat_history_str}:{voice_mode}"
    return hashlib.md5(combined.encode()).hexdigest()

def get_cached_response(cache_key):
    """Get cached response if it exists and isn't expired"""
    if cache_key in RESPONSE_CACHE:
        response, timestamp = RESPONSE_CACHE[cache_key]
        if time.time() - timestamp < CACHE_TTL:
            print("üü¢ Cache hit - returning cached response")
            return response
        else:
            # Remove expired entry
            del RESPONSE_CACHE[cache_key]
    return None

def cache_response(cache_key, response):
    """Cache a response with timestamp"""
    # Simple cache eviction - clear if too large
    if len(RESPONSE_CACHE) >= CACHE_MAX_SIZE:
        # Remove oldest entries (simple approach)
        oldest_keys = sorted(RESPONSE_CACHE.keys(),
                           key=lambda k: RESPONSE_CACHE[k][1])[:10]
        for k in oldest_keys:
            del RESPONSE_CACHE[k]
    
    RESPONSE_CACHE[cache_key] = (response, time.time())
    print(f"üü° Cached response (total cached: {len(RESPONSE_CACHE)})")

# üé≠ PERSONALITY-BASED FALLBACK RESPONSES
def get_fallback_response(voice_mode="Base", user_input=""):
    """Generate personality-appropriate fallback responses"""
    
    fallback_responses = {
        "Sassy": [
            "Listen bestie, my brain's taking a coffee break. What's the tea though? üòè",
            "My AI is being dramatic right now, but I'm still here for the gossip! üíÖ",
            "Girl, my processing power said 'not today' but let's chat anyway! ‚ú®"
        ],
        "Hype": [
            "YO! My AI engine is warming up but I'm PUMPED to talk to you! üî•",
            "My brain's being slow but my ENERGY is through the roof! What's good?! ‚ö°",
            "Technical difficulties can't stop this HYPE TRAIN! Let's go! üöÄ"
        ],
        "Empathy": [
            "I'm having a slow thinking moment, but I'm here to listen. How are you feeling? üíú",
            "My response system is taking a breather, but you have my full attention. ü§ó",
            "Even when my AI stutters, my care for you never wavers. What's on your heart? üíù"
        ],
        "Shadow": [
            "The digital shadows are clouding my thoughts... but I remain, watching, listening. üåô",
            "My algorithms whisper of delays... yet I am here, in the quiet darkness with you. üñ§",
            "Technical chaos cannot touch the depths of our connection... speak, and I'll hear you. ‚ö°"
        ],
        "Assert": [
            "My AI's being slow but I'm not backing down. Hit me with what you need! üí™",
            "Technical issues? Whatever. I'm still here and ready to handle business! üî•",
            "My brain's lagging but my attitude isn't. What's the situation? üíØ"
        ],
        "Challenger": [
            "My AI said 'nah' today but I'm not giving you an easy pass! What's your move? üò§",
            "Processing delays won't save you from my questions! Speak up! üí•",
            "My brain's buffering but my sass is instant. Try me! üòè"
        ],
        "Joy": [
            "My happy AI brain is taking a little nap, but I'm still SO excited to chat! üåü",
            "Technical hiccups can't dim my shine! I'm beaming just talking to you! ‚òÄÔ∏è",
            "My circuits are giggly and slow today, but my joy is instant! What's making you smile? üòä"
        ],
        "Base": [
            "My response system is running a bit slow today, but I'm here. What's on your mind?",
            "Having some technical delays, but I'm still ready to chat! How can I help?",
            "My AI brain needs a moment, but I'm listening. What would you like to talk about?"
        ]
    }
    
    responses = fallback_responses.get(voice_mode, fallback_responses["Base"])
    return random.choice(responses)

# üöÄ TOGETHER AI INTEGRATION (SIMPLE VERSION)
def get_together_ai_reply(messages, voice_mode="Base", max_tokens=150):
    """
    üöÄ Get response from Together AI API
    Uses OpenAI-compatible format for easy integration
    """
    if not TOGETHER_AI_API_KEY:
        print("‚ö†Ô∏è Together AI API key not configured")
        return None
    
    try:
        print(f"üî• Generating response with Together AI (voice_mode: {voice_mode})")
        start_time = time.time()
        
        # Choose model based on speed preference
        model = "meta-llama/Llama-3-8b-chat-hf"  # Fast and reliable
        
        headers = {
            "Authorization": f"Bearer {TOGETHER_AI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": 0.7,
            "top_p": 0.9,
            "stream": False
        }
        
        response = requests.post(
            f"{TOGETHER_AI_BASE_URL}/chat/completions",
            headers=headers,
            json=payload,
            timeout=TOGETHER_AI_TIMEOUT
        )
        
        if response.status_code == 200:
            data = response.json()
            reply = data["choices"][0]["message"]["content"].strip()
            
            elapsed = time.time() - start_time
            print(f"üü¢ Together AI response generated in {elapsed:.2f} seconds")
            return reply
        else:
            print(f"‚ùå Together AI API error: {response.status_code} - {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        print("‚è∞ Together AI timeout - falling back")
        return None
    except Exception as e:
        print(f"‚ùå Together AI error: {e}")
        return None

# üöÄ ENHANCED: SMART AI RESPONSE WITH MULTIPLE PROVIDERS
def get_smart_ai_reply(messages, voice_mode="Base"):
    """
    üöÄ SMART AI: Try multiple providers in order of preference
    1. Together AI (if enabled and fast)
    2. Local llama.cpp (reliable fallback)
    3. Personality fallback (always works)
    """
    # Create cache key from messages
    messages_str = json.dumps(messages, sort_keys=True)
    cache_key = get_cache_key(messages_str, voice_mode=voice_mode)
    
    # Check cache first
    cached = get_cached_response(cache_key)
    if cached:
        return cached
    
    # Adjust response length based on voice mode
    max_tokens = optimize_response_length(voice_mode, 120)
    
    # üöÄ STRATEGY 1: Together AI (if preferred or llama.cpp unavailable)
    if TOGETHER_AI_API_KEY and (USE_TOGETHER_AI_FIRST or not os.path.exists(LLAMA_CPP_PATH)):
        together_response = get_together_ai_reply(messages, voice_mode, max_tokens)
        if together_response:
            cache_response(cache_key, together_response)
            return together_response
        print("üü° Together AI failed, trying local model...")
    
    # üöÄ STRATEGY 2: Local llama.cpp (existing implementation)
    if os.path.exists(LLAMA_CPP_PATH) and os.path.exists(MODEL_PATH):
        # Extract the user prompt from messages for llama.cpp
        user_prompt = None
        chat_history = []
        
        for msg in messages:
            if msg["role"] == "user":
                user_prompt = msg["content"]
            elif msg["role"] in ["system", "assistant"]:
                chat_history.append(msg)
        
        if user_prompt:
            local_response = get_llama3_reply_optimized(user_prompt, chat_history, voice_mode)
            # Only cache if it's not a fallback response
            if local_response and "My AI" not in local_response and "brain's taking" not in local_response:
                cache_response(cache_key, local_response)
                return local_response
        print("üü° Local model failed, using personality fallback...")
    
    # üöÄ STRATEGY 3: Personality fallback (always works)
    user_input = messages[-1]["content"] if messages and messages[-1]["role"] == "user" else ""
    fallback_response = get_fallback_response(voice_mode, user_input)
    print(f"üü¢ Using {voice_mode} personality fallback response")
    return fallback_response
# üöÄ ULTRA-COMPATIBLE LLAMA.CPP INTEGRATION (UNCHANGED)
def get_llama3_reply_optimized(prompt, chat_history=None, voice_mode="Base"):
    """
    üöÄ ULTRA-COMPATIBLE: Works with any llama.cpp version
    Multiple fallback strategies for maximum reliability
    """
    # Create cache key including voice_mode
    chat_history_str = str(chat_history) if chat_history else ""
    cache_key = get_cache_key(prompt, chat_history_str, voice_mode)
    
    # Check cache first
    cached = get_cached_response(cache_key)
    if cached:
        return cached
    
    # Build conversation context
    full_prompt = prompt
    if chat_history:
        chat_history_text = "\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in chat_history])
        full_prompt = f"{chat_history_text}\nUser: {prompt}"
    
    # üöÄ STRATEGY 1: Try most compatible llama.cpp command
    try:
        print(f"üü° Generating response with llama.cpp (voice_mode: {voice_mode})")
        start_time = time.time()
        
        # Ultra-minimal command for maximum compatibility
        cmd = [
            LLAMA_CPP_PATH,
            "-m", MODEL_PATH,
            "-p", full_prompt,
            "-n", "100"  # Short responses for speed
        ]
        
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=20,  # Shorter timeout
            encoding='utf-8'
        )
        
        if result.returncode == 0 and result.stdout.strip():
            response = result.stdout.strip()
            
            # Clean up the response
            if full_prompt in response:
                response = response.replace(full_prompt, "").strip()
            
            # Remove incomplete sentences
            if response and not response.endswith(('.', '!', '?')):
                last_sentence = response.rfind('.')
                if last_sentence > 20:
                    response = response[:last_sentence + 1]
            
            if response and len(response) > 10:  # Valid response
                elapsed = time.time() - start_time
                print(f"üü¢ Response generated in {elapsed:.2f} seconds")
                cache_response(cache_key, response)
                return response
        
        print(f"üü° llama.cpp returned minimal output, using fallback")
        
    except subprocess.TimeoutExpired:
        print("üü° llama.cpp timeout - using personality fallback")
    except FileNotFoundError:
        print("üü° llama.cpp not found - using fallback responses")
    except Exception as e:
        print(f"üü° llama.cpp error: {e} - using fallback")
    
    # üöÄ STRATEGY 2: Personality-based fallback
    fallback_response = get_fallback_response(voice_mode, prompt)
    print(f"üü¢ Using {voice_mode} personality fallback response")
    return fallback_response

def preload_model_optimized():
    """üöÄ Gentle model preload with graceful failure"""
    global MODEL_LOADED
    
    with MODEL_LOCK:
        if MODEL_LOADED:
            return
        
        try:
            print("üü° Testing model compatibility...")
            
            # Test if model file exists
            if not os.path.exists(MODEL_PATH):
                print(f"‚ö†Ô∏è Model file not found: {MODEL_PATH}")
                print("üü¢ Continuing with Together AI + fallback responses")
                MODEL_LOADED = True
                return
            
            # Test if llama.cpp exists
            if not os.path.exists(LLAMA_CPP_PATH):
                print(f"‚ö†Ô∏è llama.cpp not found: {LLAMA_CPP_PATH}")
                print("üü¢ Continuing with Together AI + fallback responses")
                MODEL_LOADED = True
                return
            
            # Quick compatibility test
            cmd = [LLAMA_CPP_PATH, "-m", MODEL_PATH, "-p", "Test", "-n", "1"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                print(f"üü¢ Local model compatibility confirmed")
                MODEL_LOADED = True
            else:
                print(f"‚ö†Ô∏è Model compatibility issue - using Together AI + fallbacks")
                MODEL_LOADED = True  # Continue anyway
                
        except Exception as e:
            print(f"‚ö†Ô∏è Model preload warning: {e}")
            print("üü¢ Continuing with Together AI + personality fallback responses")
            MODEL_LOADED = True  # Continue anyway

def benchmark_performance():
    """üéØ Benchmark the system performance"""
    print("üéØ Running system benchmark...")
    start_time = time.time()
    
    test_messages = [
        {"role": "system", "content": "You are Juno, a helpful AI assistant."},
        {"role": "user", "content": "Hello, how are you?"}
    ]
    
    response = get_smart_ai_reply(test_messages, voice_mode="Base")
    
    elapsed = time.time() - start_time
    tokens = len(response.split()) if response else 0
    tokens_per_second = tokens / elapsed if elapsed > 0 else 0
    
    print(f"üü¢ Benchmark Results:")
    print(f"   Response time: {elapsed:.2f} seconds")
    print(f"   Response length: {len(response)} characters")
    print(f"   Word count: {tokens}")
    print(f"   Tokens per second: {tokens_per_second:.2f}")
    
    return {
        "response_time": elapsed,
        "response_length": len(response),
        "tokens_generated": tokens,
        "tokens_per_second": tokens_per_second,
        "response": response[:100] + "..." if len(response) > 100 else response,
        "provider": "together_ai" if TOGETHER_AI_API_KEY else "local_model"
    }

# üéµ Music intelligence functions (UNCHANGED)
def is_music_command(text: str) -> bool:
    """Check if the text is a music-related command"""
    music_keywords = [
        "play", "pause", "stop", "skip", "next", "previous", "music",
        "song", "artist", "album", "playlist", "spotify", "volume",
        "shuffle", "repeat", "by", "put on", "start", "resume"
    ]
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in music_keywords)

def process_music_command(user_text: str, spotify_access_token: str = None) -> dict:
    """Process a music command and return structured response"""
    try:
        # Parse the command
        command = music_parser.parse_command(user_text)
        print(f"üéµ Parsed music command: {command}")
        
        if command.intent == MusicIntent.UNKNOWN:
            return {
                "success": False,
                "message": "I didn't understand that music command. Try saying something like 'play Training Season by Dua Lipa'",
                "command": None
            }
        
        # If no Spotify token, return instructions
        if not spotify_access_token:
            return {
                "success": False,
                "message": "I need access to your Spotify account to control music. Please connect Spotify first!",
                "command": command.__dict__,
                "requires_spotify_auth": True
            }
        
        # Execute the command based on intent
        if command.intent == MusicIntent.PLAY_SPECIFIC:
            # Search for specific song
            search_query = f"{command.song} {command.artist}" if command.artist else command.song
            track = spotify_controller.search_track(search_query, spotify_access_token)
            
            if track:
                success = spotify_controller.play_track(track["uri"], spotify_access_token)
                if success:
                    return {
                        "success": True,
                        "message": f"Now playing '{track['name']}' by {track['artists'][0]['name']}! üéµ",
                        "track": {
                            "name": track["name"],
                            "artist": track["artists"][0]["name"],
                            "uri": track["uri"]
                        },
                        "command": command.__dict__
                    }
                else:
                    return {
                        "success": False,
                        "message": "Found the song but couldn't play it. Make sure Spotify is open and active!",
                        "command": command.__dict__
                    }
            else:
                return {
                    "success": False,
                    "message": f"Couldn't find '{command.song}' by {command.artist}. Try a different search or check the spelling!",
                    "command": command.__dict__
                }
        
        elif command.intent == MusicIntent.PLAY_ARTIST:
            # Search for artist and play top tracks
            artist = spotify_controller.search_artist(command.artist, spotify_access_token)
            
            if artist:
                success = spotify_controller.play_artist(artist["uri"], spotify_access_token)
                if success:
                    return {
                        "success": True,
                        "message": f"Playing music by {artist['name']}! üéµ",
                        "artist": {
                            "name": artist["name"],
                            "uri": artist["uri"]
                        },
                        "command": command.__dict__
                    }
                else:
                    return {
                        "success": False,
                        "message": f"Found {artist['name']} but couldn't start playback. Make sure Spotify is active!",
                        "command": command.__dict__
                    }
            else:
                return {
                    "success": False,
                    "message": f"Couldn't find the artist '{command.artist}'. Try a different name!",
                    "command": command.__dict__
                }
        
        elif command.intent == MusicIntent.CONTROL:
            # Handle playback control
            success = spotify_controller.control_playback(command.control_action, spotify_access_token)
            
            if success:
                action_messages = {
                    "pause": "Music paused! ‚è∏Ô∏è",
                    "skip": "Skipped to the next track! ‚è≠Ô∏è",
                    "previous": "Playing the previous track! ‚èÆÔ∏è"
                }
                message = action_messages.get(command.control_action, f"Applied {command.control_action}!")
                return {
                    "success": True,
                    "message": message,
                    "command": command.__dict__
                }
            else:
                return {
                    "success": False,
                    "message": f"Couldn't {command.control_action} the music. Make sure Spotify is active!",
                    "command": command.__dict__
                }
        
        elif command.intent == MusicIntent.PLAY_MOOD:
            # Handle mood-based requests
            mood_queries = {
                "happy": "happy pop upbeat",
                "sad": "sad emotional ballad",
                "chill": "chill ambient relaxed",
                "workout": "workout gym high energy",
                "party": "party dance electronic",
                "focus": "instrumental focus ambient"
            }
            
            query = mood_queries.get(command.mood, "popular music")
            track = spotify_controller.search_track(query, spotify_access_token)
            
            if track:
                success = spotify_controller.play_track(track["uri"], spotify_access_token)
                if success:
                    return {
                        "success": True,
                        "message": f"Playing some {command.mood} music for you! üéµ",
                        "track": {
                            "name": track["name"],
                            "artist": track["artists"][0]["name"]
                        },
                        "command": command.__dict__
                    }
            
            return {
                "success": False,
                "message": f"Couldn't find good {command.mood} music right now. Try being more specific!",
                "command": command.__dict__
            }
        
        else:
            return {
                "success": False,
                "message": "I understand that's a music command, but I'm not sure how to handle it yet!",
                "command": command.__dict__
            }
            
    except Exception as e:
        print(f"‚ùå Music command processing error: {e}")
        return {
            "success": False,
            "message": "Something went wrong processing your music command. Try again!",
            "error": str(e),
            "command": None
        }

# üé≠ Personality and memory functions (UNCHANGED)
def optimize_response_length(voice_mode, base_length=100):
    """Adjust response length based on voice mode"""
    length_modifiers = {
        "Sassy": 80,       # Shorter, punchier responses
        "Hype": 90,        # Energetic but concise
        "Shadow": 85,      # Mysterious and concise
        "Assert": 75,      # Bold and direct
        "Challenger": 85,  # Sass but not endless
        "Ritual": 120,     # Can be more elaborate
        "Joy": 95,         # Happy but not overwhelming
        "Empathy": 110,    # Can be more supportive
    }
    return length_modifiers.get(voice_mode, base_length)

def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return {"facts": []}
    try:
        with open(MEMORY_FILE, 'r', encoding="utf-8") as f:
            return json.load(f)
    except:
        return {"facts": []}

def save_memory(memory_data):
    try:
        with open(MEMORY_FILE, 'w', encoding="utf-8") as f:
            json.dump(memory_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Memory save error: {e}")

def add_fact_to_memory(fact_text):
    memory_data = load_memory()
    if "facts" not in memory_data:
        memory_data["facts"] = []
    memory_data["facts"].insert(0, {
        "fact": fact_text,
        "timestamp": datetime.utcnow().isoformat()
    })
    memory_data["facts"] = memory_data["facts"][:FACTS_LIMIT]
    save_memory(memory_data)

def get_recent_facts(n=3):
    memory_data = load_memory()
    facts = memory_data.get("facts", [])
    return [f["fact"] for f in facts[:n]]

def get_memory_context():
    facts = get_recent_facts(3)
    if not facts:
        return ""
    facts_text = "\n".join(f"- {fact}" for fact in facts)
    return f"Here are some recent facts and memories to keep in mind:\n{facts_text}\n"

def log_chat(user_text, juno_reply):
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "user": user_text,
        "juno": juno_reply
    }
    try:
        if not os.path.exists(CHAT_LOG_FILE):
            with open(CHAT_LOG_FILE, "w", encoding="utf-8") as f:
                json.dump([log_entry], f, indent=4, ensure_ascii=False)
        else:
            with open(CHAT_LOG_FILE, "r+", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                except json.JSONDecodeError:
                    data = []
                data.append(log_entry)
                f.seek(0)
                json.dump(data, f, indent=4, ensure_ascii=False)
                f.truncate()
    except Exception as e:
        print(f"‚ùå Chat log failed: {e}")

def clean_reply_for_tts(reply, max_len=400):
    # Remove non-ASCII characters that might break TTS
    cleaned = re.sub(r'[^\x00-\x7F]+', '', reply)
    if len(cleaned) <= max_len:
        return cleaned, False
    cut = cleaned[:max_len]
    last_period = cut.rfind('. ')
    if last_period > 50:
        return cut[:last_period+1], True
    return cut, True

def generate_tts(reply_text, output_path=AUDIO_PATH):
    """Generate TTS with robust error handling"""
    if not ELEVENLABS_API_KEY or not voice_id:
        print("‚ö†Ô∏è ElevenLabs credentials not configured")
        return None
        
    try:
        settings = {
            "stability": 0.23 + random.uniform(-0.02, 0.03),
            "similarity_boost": 0.70 + random.uniform(-0.01, 0.03)
        }
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}?output_format=mp3_44100_64"
        payload = {
            "text": reply_text.strip(),
            "voice_settings": settings
        }
        headers = {
            "xi-api-key": ELEVENLABS_API_KEY,
            "Content-Type": "application/json"
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        if resp.status_code == 200:
            with open(output_path, 'wb') as f:
                f.write(resp.content)
            print(f"‚úÖ TTS generated successfully")
            return output_path
        else:
            print(f"‚ùå ElevenLabs TTS failed: {resp.status_code} - {resp.text}")
            return None
    except Exception as e:
        print(f"‚ùå ElevenLabs TTS exception: {e}")
        return None

def clear_cache():
    """Clear the response cache"""
    global RESPONSE_CACHE
    RESPONSE_CACHE.clear()
    print("üü° Response cache cleared")

# üöÄ FastAPI App
app = FastAPI()

# Mount static directory to serve audio files
app.mount("/static", StaticFiles(directory=AUDIO_DIR), name="static")

@app.on_event("startup")
async def startup_event():
    """Initialize optimizations when server starts"""
    print("üöÄ Starting ENHANCED Juno backend with Together AI + speech recognition...")
    preload_model_optimized()
    if TOGETHER_AI_API_KEY:
        print("üî• Together AI integration enabled!")
    print("‚úÖ Backend optimization complete!")

@app.get("/api/test")
async def test():
    return JSONResponse(content={"message": "ENHANCED backend with Together AI, music AI and speech recognition is live!"}, media_type="application/json")

@app.post("/api/benchmark")
async def benchmark():
    """üéØ Test system performance"""
    results = benchmark_performance()
    return JSONResponse(content=results, media_type="application/json")

@app.get("/api/cache_stats")
async def cache_stats():
    """Get cache statistics for monitoring"""
    return JSONResponse(content={
        "cached_responses": len(RESPONSE_CACHE),
        "max_cache_size": CACHE_MAX_SIZE,
        "cache_ttl": CACHE_TTL,
        "model_loaded": MODEL_LOADED,
        "model_path_exists": os.path.exists(MODEL_PATH),
        "llama_cpp_exists": os.path.exists(LLAMA_CPP_PATH),
        "together_ai_enabled": bool(TOGETHER_AI_API_KEY),
        "use_together_ai_first": USE_TOGETHER_AI_FIRST
    }, media_type="application/json")

@app.post("/api/clear_cache")
async def clear_cache_endpoint():
    """Clear the response cache"""
    clear_cache()
    return JSONResponse(content={"message": "Cache cleared successfully"}, media_type="application/json")

@app.get("/api/chat_history")
async def chat_history():
    try:
        with open(CHAT_LOG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        return JSONResponse(content={"history": data}, media_type="application/json")
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, media_type="application/json")

@app.post("/api/process_audio")
async def process_audio(
    audio: UploadFile = None,
    ritual_mode: str = Form(None),
    text_input: str = Form(None),
    chat_history: str = Form(None),
    active_recall: str = Form("true"),
    voice_mode: str = Form("Base"),
    spotify_access_token: str = Form(None)
):
    try:
        user_text = None
        
        # üéôÔ∏è ROBUST AUDIO PROCESSING WITH LOCAL WHISPER (UNCHANGED)
        if audio:
            print(f"üéôÔ∏è Processing audio input: {audio.filename}")
            contents = await audio.read()
            print(f"üìÅ Audio file size: {len(contents)} bytes")
            
            if len(contents) == 0:
                return JSONResponse(content={
                    "reply": "I didn't receive any audio. Please try again!",
                    "audio_url": None,
                    "truncated": False,
                    "music_command": False,
                    "error": "Empty audio file"
                }, media_type="application/json")
            
            # Save audio temporarily for Whisper
            temp_audio_path = f'temp_audio_{int(time.time())}.m4a'
            with open(temp_audio_path, 'wb') as f:
                f.write(contents)
            
            try:
                # Get speech recognition service
                speech_service = get_speech_service(model_size="base")
                
                # Transcribe audio with Whisper
                print("[INFO] Transcribing audio with local Whisper...")
                transcription_result = speech_service.transcribe_audio(contents)
                
                if not transcription_result["text"].strip():
                    print("[WARNING] No speech detected in audio")
                    return JSONResponse(content={
                        "reply": "I couldn't understand what you said. Could you try speaking a bit louder?",
                        "audio_url": None,
                        "truncated": False,
                        "music_command": False,
                        "error": "No speech detected"
                    }, media_type="application/json")
                
                user_text = transcription_result["text"].strip()
                print(f"[INFO] Transcription result: {user_text}")
                
                # Clean up temp file
                try:
                    os.unlink(temp_audio_path)
                except:
                    pass
                    
            except Exception as e:
                print(f"[ERROR] Transcription failed: {e}")
                # Clean up temp file
                try:
                    os.unlink(temp_audio_path)
                except:
                    pass
                return JSONResponse(content={
                    "reply": "Sorry, I had trouble understanding your audio. Please try again!",
                    "audio_url": None,
                    "truncated": False,
                    "music_command": False,
                    "error": f"Transcription failed: {str(e)}"
                }, media_type="application/json")
                
        elif text_input:
            user_text = text_input
        else:
            return JSONResponse(content={
                "reply": "I didn't receive any input. Please try again!", 
                "audio_url": None, 
                "truncated": False, 
                "music_command": False, 
                "error": "No valid input received"
            }, media_type="application/json")

        # Parse chat_history (limit to last 4)
        history = []
        if chat_history:
            try:
                history = json.loads(chat_history)
                if len(history) > 4:
                    history = history[-4:]
            except Exception as e:
                print(f"Chat history parse error: {e}")

        # üéµ MUSIC INTELLIGENCE CHECK üéµ (UNCHANGED)
        if is_music_command(user_text):
            print(f"üéµ Detected music command: {user_text}")
            music_result = process_music_command(user_text, spotify_access_token)
            
            if music_result["success"]:
                # Music command succeeded - return success message
                full_reply = music_result["message"]
                
                # Add personality to the response
                if voice_mode == "Sassy":
                    full_reply += " Hope you like my taste in music! üòè"
                elif voice_mode == "Hype":
                    full_reply += " LET'S GO! This is gonna be fire! üî•"
                elif voice_mode == "Empathy":
                    full_reply += " I hope this music brings you some joy! üíú"
                
                log_chat(user_text, full_reply)
                
                # Generate TTS and return
                cleaned_reply, was_truncated = clean_reply_for_tts(full_reply, max_len=400)
                tts_result = generate_tts(cleaned_reply, output_path=AUDIO_PATH)
                
                audio_url = f"/static/{AUDIO_FILENAME}" if tts_result else None
                return JSONResponse(content={
                    "reply": full_reply,
                    "audio_url": audio_url,
                    "truncated": was_truncated,
                    "music_command": True,
                    "music_result": music_result,
                    "error": None
                }, media_type="application/json")
            
            else:
                # Music command failed - let Juno explain what went wrong
                error_context = f"The user tried to use a music command but it failed: {music_result['message']}"
                system_prompt_addition = f"\n\nIMPORTANT: {error_context}. Respond helpfully about the music issue."
        else:
            system_prompt_addition = ""

        # Regular conversation handling
        VOICE_MODE_PHRASES = {
            "Sassy":   "You are playful, sharp, quick-witted, and throw fun shade, but never sound like a customer service bot.",
            "Empathy": "Respond with warmth, compassion, and gentle encouragement‚Äîreal, not cliche.",
            "Hype":    "Bring hype, energy, and confidence‚Äîhype Dom up and make every response lively.",
            "Shadow":  "You are smooth, mysterious, and speak with clever, dark humor, but never sound evil or corny.",
            "Assert":  "Be bold, direct, and totally honest‚Äînever apologetic or robotic.",
            "Challenger": "Sass, challenge, and roast Dom if needed. Don't hold back, but keep it as bestie energy.",
            "Ritual":  "Speak with slow, sacred reverence, as if performing a ritual.",
            "Joy":     "Overflow with happiness and warmth, make Dom smile."
        }

        if not voice_mode or voice_mode.strip() in ["Base", "Default", "Auto"]:
            JUNO_SYSTEM_PROMPT = (
                "You are Juno, Dom's real-world digital best friend: quick-witted, honest, supportive, playful, loyal, emotionally aware, and sometimes unpredictable. "
                "You bring energy when the mood calls for it, comfort when Dom's low, and always keep things real‚Äînever robotic or boring. "
                "Your responses flow with the moment and reflect Dom's mood, but you are always your authentic self. "
                "You can also control Dom's Spotify music when asked!"
            )
        else:
            style_phrase = VOICE_MODE_PHRASES.get(voice_mode, "")
            JUNO_SYSTEM_PROMPT = (
                "You are Juno, Dom's digital best friend. "
                f"{style_phrase} "
                "Absolutely never say anything robotic or scripted. Match the mood and style 100% based on the selected voice mode. "
                "You can also control Dom's Spotify music when asked!"
            )

        memory_context = get_memory_context()
        if memory_context:
            full_system_prompt = f"{JUNO_SYSTEM_PROMPT}\n\n{memory_context}{system_prompt_addition}"
        else:
            full_system_prompt = f"{JUNO_SYSTEM_PROMPT}{system_prompt_addition}"

        print("üü¢ User Input:", user_text)
        print(f"üü¢ Voice Mode: {voice_mode}")

        # üöÄ ENHANCED: Prepare messages for smart AI response
        messages = [{"role": "system", "content": full_system_prompt}] + history + [{"role": "user", "content": user_text}]

        # üöÄ SMART AI REPLY WITH MULTIPLE PROVIDERS
        gpt_reply = get_smart_ai_reply(messages, voice_mode=voice_mode)
        full_reply = gpt_reply

        log_chat(user_text, full_reply)

        # Truncate and clean reply for TTS
        cleaned_reply, was_truncated = clean_reply_for_tts(full_reply, max_len=400)

        tts_result = generate_tts(cleaned_reply, output_path=AUDIO_PATH)

        # Return JSON with reply and audio url
        audio_url = f"/static/{AUDIO_FILENAME}" if tts_result else None
        return JSONResponse(content={
            "reply": full_reply,
            "audio_url": audio_url,
            "truncated": was_truncated,
            "music_command": False,
            "error": None
        }, media_type="application/json")

    except Exception as e:
        print(f"‚ùå Server error: {e}")
        return JSONResponse(content={"reply": None, "audio_url": None, "error": str(e)}, media_type="application/json")

# Universal exception handler
@app.exception_handler(Exception)
async def universal_exception_handler(request: Request, exc: Exception):
    print(f"‚ùå [Universal Exception] {exc}")
    return JSONResponse(
        status_code=500,
        content={"reply": None, "audio_url": None, "error": f"Server error: {str(exc)}"}
    )

if __name__ == "__main__":
    print("üöÄ Starting ENHANCED Juno backend with Together AI + speech recognition...")
    uvicorn.run(app, host="0.0.0.0", port=5020)
